{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import draw\n",
    "import data_Manager\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from HyperParameters import HP\n",
    "import train as tu\n",
    "import generate as g\n",
    "model_name = \"model_weight_carrot_50_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 02:28:47.802174 139985304876864 deprecation.py:506] From /home/mario/git/Sketch-rnn/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "decoder_init_stat (Dense)    (None, 1024)              263168    \n",
      "=================================================================\n",
      "Total params: 263,168\n",
      "Trainable params: 263,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model for predicting the inital state \n",
    "batch_z = tf.keras.Input(shape=(HP.latent_dim,))\n",
    "initial_state = tf.keras.layers.Dense(units=(2*HP.dec_hidden_size), activation='tanh', name = \"decoder_init_stat\")(batch_z)\n",
    "latent_to_hidden_state_model = tf.keras.Model(inputs=batch_z, outputs=initial_state)\n",
    "latent_to_hidden_state_model.load_weights(\"model/\"+model_name+\".h5\", by_name = True)\n",
    "latent_to_hidden_state_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 261)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_decoder (LSTM)             [(None, 1, 512), (No 1585152     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1, 123)       63099       LSTM_decoder[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,648,251\n",
      "Trainable params: 1,648,251\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the LSTM for generating\n",
    "\"\"\"\n",
    "We have 3 input tensor. The input of the LSTM and the hidden states \n",
    "\"\"\"\n",
    "decoder_input = tf.keras.Input(shape=(1, 5 + HP.latent_dim))\n",
    "initial_h_input = tf.keras.Input(shape=(HP.dec_hidden_size,))\n",
    "initial_c_input = tf.keras.Input(shape=(HP.dec_hidden_size,))\n",
    "# now the LSTM\n",
    "decoderLSTM = tf.keras.layers.LSTM(HP.dec_hidden_size, recurrent_dropout=HP.rec_dropout, \n",
    "                                    return_sequences=True, return_state=True, name = \"LSTM_decoder\")\n",
    "\n",
    "# creation of the LSTM\n",
    "decoder_output, h_new, c_new = decoderLSTM(decoder_input, initial_state = [initial_h_input, initial_c_input])\n",
    "# dense to output. THe dimention is, as explained in the paper equal to 3 + 6*M\n",
    "# 6 times M= number of mixture \n",
    "output_dimention = (3 + HP.M * 6)\n",
    "distribution_output = tf.keras.layers.Dense(output_dimention, name = \"output_layer\")(decoder_output)\n",
    "\n",
    "# Now we load the weights from the trained model\n",
    "generator = tf.keras.models.Model([decoder_input, initial_h_input, initial_c_input], outputs =[ distribution_output , h_new, c_new])\n",
    "generator.summary()\n",
    "generator.load_weights(\"model/\"+model_name+\".h5\", by_name = True)\n",
    "generator.build(tf.TensorShape([1, None])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 200, 5)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BI_LSTM_encoder (Bidirectional) (None, 512)          536576      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mean_MLP (Dense)                (None, 256)          131328      BI_LSTM_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "variance_MLP (Dense)            (None, 256)          131328      BI_LSTM_encoder[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 799,232\n",
      "Trainable params: 799,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the encoder that embed the z\n",
    "\"\"\"\n",
    "encoder_input = tf.keras.layers.Input(shape = (HP.max_seq_length, HP.input_dimention), name = \"encoder_input\" )\n",
    "\n",
    "encoderLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HP.enc_hidden_size, return_sequences=False,\n",
    "    recurrent_dropout=HP.rec_dropout, name = \"LSTM_encoder\"), merge_mode='concat', name = \"BI_LSTM_encoder\")(encoder_input)\n",
    "\n",
    "hidden_state_mean = tf.keras.layers.Dense(HP.latent_dim, activation='linear', name = \"mean_MLP\")(encoderLSTM)\n",
    "\n",
    "hidden_state_variance = tf.keras.layers.Dense(HP.latent_dim, activation='linear', name = \"variance_MLP\")(encoderLSTM)\n",
    "# Now we load the weights from the trained model\n",
    "encoder = tf.keras.models.Model(encoder_input,[hidden_state_mean, hidden_state_variance])\n",
    "encoder.summary()\n",
    "encoder.load_weights(\"model/\"+model_name+\".h5\", by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP.data_location = \"data/carrot.npz\"\n",
    "datas = data_Manager.Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"147.67015414916176\" version=\"1.1\" width=\"80.12257090581625\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"147.67015414916176\" width=\"80.12257090581625\" x=\"0\" y=\"0\"/><path d=\"M40.28948674764916,99.3936220855765 m0.0,0.0 m10.269058263346446,-41.760836937608886 l-5.020428484302708,-0.9128051789641286 -15.061285452908123,0.4564025894820643 l-4.564025894820643,1.8256103579282572 -0.9128051789641286,2.966616831633418 l2.510214242151354,10.040856968605416 10.953662147569544,22.36372688462115 l3.6512207158565144,18.484304874023604 3.6512207158565144,9.812655673864382 l2.510214242151354,-2.0538116526692893 1.8256103579282572,-4.3358246000796115 l4.564025894820643,-16.65869451609535 -0.6846038842230965,-20.081713937210832 l1.1410064737051608,-9.356253084382319 -0.4564025894820643,-4.3358246000796115 l-2.738415536892386,-5.020428484302708 -3.19481812637445,-2.966616831633418 l-2.2820129474103217,-0.6846038842230965 m-12.322869916015737,1.1410064737051608 l0.0,-1.1410064737051608 -7.530642726454062,-14.148480273943994 l-1.369207768446193,-5.476831073784772 -0.22820129474103215,-5.476831073784772 l3.6512207158565144,2.510214242151354 7.987045315936125,8.44344790541819 l2.2820129474103217,0.9128051789641286 0.9128051789641286,-2.0538116526692893 l0.9128051789641286,-10.269058263346446 0.9128051789641286,-3.19481812637445 l2.2820129474103217,-2.966616831633418 1.369207768446193,-0.4564025894820643 l1.1410064737051608,1.369207768446193 0.22820129474103215,2.738415536892386 l-1.8256103579282572,19.1689087582467 0.0,9.58445437912335 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "x = datas.train[random.randint(0, len(x))]\n",
    "x_2 = np.expand_dims(x, axis = 0)\n",
    "mean, variance = encoder.predict(x_2)\n",
    "latent = np.random.rand(1,HP.latent_dim)*np.exp(variance/2)+mean\n",
    "draw.draw_strokes(x, svg_filename=\"stroke_1.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of sketch\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"140.7610498059803\" version=\"1.1\" width=\"87.66908372827739\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"140.7610498059803\" width=\"87.66908372827739\" x=\"0\" y=\"0\"/><path d=\"M30.26840807374716,61.69407187864837 m1.474570816111526,13.375281700106486 l4.609727999365034,15.44504091105137 8.654451209927384,20.148444936851348 l2.3640232959932894,4.588730801458492 0.9060619876879574,0.5094795778642308 l4.494384349835865,-12.740953151262412 3.0366458858225407,-18.194125941202728 l4.203662734159823,-10.171756116494022 2.1036119682894183,-7.545056357380037 l0.10857589767691407,-5.629209515623385 -0.8971752966708962,-1.4198683941439953 l-4.485397837870427,-0.5944659193256039 -10.582398170022005,0.11531769532994132 l-12.968045144439774,0.23800874702451363 -1.511982600672502,2.2200989889231733 m10.000374517279326,-2.4849874667441387 l0.11466592848770829,-2.220581426177083 -1.5827468018968311,-3.396463165968201 l-5.702433484091199,-8.408269402369735 -4.233963669266291,-4.382306452782314 l-5.375021659454024,-0.581545823054391 5.060530518668731,2.928714442494598 l6.144245437881259,8.763484710913431 2.4609213824992726,7.785414683482312 l0.7736168720820913,-12.51220758693128 3.768581390520549,-11.719789060162912 l2.766120765878195,-5.3866139274608305 4.122495454942062,-5.423889367065202 l-5.009106811100836,12.576556876926848 -1.247978328486975,15.81668154053594 l3.4953717293301887,-1.6172894540213034 6.102982765027405,-10.003570348887271 l3.7632496031969884,-3.2718124735146095 5.468052947838462,0.8674751336023173 l-10.260512472011998,5.931514822080302 -4.126091789436941,4.674584432396164 l-3.4217255421582076,8.154235555531088 -0.691002209173685,4.399021740723861 l4.487011097209443,-4.089272537028738 m0.0,0.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq =  g.generate_sketch(generator, latent_to_hidden_state_model, latent, temperature=0.2)\n",
    "draw.draw_strokes(seq, svg_filename=\"results/carrot_from_latent.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import draw\n",
    "import data_Manager\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from HyperParameters import HP\n",
    "import train as tu\n",
    "import generate as g\n",
    "model_name = \"model_weight_carrot_50_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "decoder_init_stat (Dense)    (None, 1024)              263168    \n",
      "=================================================================\n",
      "Total params: 263,168\n",
      "Trainable params: 263,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model for predicting the inital state \n",
    "batch_z = tf.keras.Input(shape=(HP.latent_dim,))\n",
    "initial_state = tf.keras.layers.Dense(units=(2*HP.dec_hidden_size), activation='tanh', name = \"decoder_init_stat\")(batch_z)\n",
    "latent_to_hidden_state_model = tf.keras.Model(inputs=batch_z, outputs=initial_state)\n",
    "latent_to_hidden_state_model.load_weights(\"model/\"+model_name+\".h5\", by_name = True)\n",
    "latent_to_hidden_state_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 1, 261)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_decoder (LSTM)             [(None, 1, 512), (No 1585152     input_6[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1, 123)       63099       LSTM_decoder[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,648,251\n",
      "Trainable params: 1,648,251\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the LSTM for generating\n",
    "\"\"\"\n",
    "We have 3 input tensor. The input of the LSTM and the hidden states \n",
    "\"\"\"\n",
    "decoder_input = tf.keras.Input(shape=(1, 5 + HP.latent_dim))\n",
    "initial_h_input = tf.keras.Input(shape=(HP.dec_hidden_size,))\n",
    "initial_c_input = tf.keras.Input(shape=(HP.dec_hidden_size,))\n",
    "# now the LSTM\n",
    "decoderLSTM = tf.keras.layers.LSTM(HP.dec_hidden_size, recurrent_dropout=HP.rec_dropout, \n",
    "                                    return_sequences=True, return_state=True, name = \"LSTM_decoder\")\n",
    "\n",
    "# creation of the LSTM\n",
    "decoder_output, h_new, c_new = decoderLSTM(decoder_input, initial_state = [initial_h_input, initial_c_input])\n",
    "# dense to output. THe dimention is, as explained in the paper equal to 3 + 6*M\n",
    "# 6 times M= number of mixture \n",
    "output_dimention = (3 + HP.M * 6)\n",
    "distribution_output = tf.keras.layers.Dense(output_dimention, name = \"output_layer\")(decoder_output)\n",
    "\n",
    "# Now we load the weights from the trained model\n",
    "generator = tf.keras.models.Model([decoder_input, initial_h_input, initial_c_input], outputs =[ distribution_output , h_new, c_new])\n",
    "generator.summary()\n",
    "generator.load_weights(\"model/\"+model_name+\".h5\", by_name = True)\n",
    "generator.build(tf.TensorShape([1, None])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 200, 5)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BI_LSTM_encoder (Bidirectional) (None, 512)          536576      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mean_MLP (Dense)                (None, 256)          131328      BI_LSTM_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "variance_MLP (Dense)            (None, 256)          131328      BI_LSTM_encoder[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 799,232\n",
      "Trainable params: 799,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the encoder that embed the z\n",
    "\"\"\"\n",
    "encoder_input = tf.keras.layers.Input(shape = (HP.max_seq_length, HP.input_dimention), name = \"encoder_input\" )\n",
    "\n",
    "encoderLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HP.enc_hidden_size, return_sequences=False,\n",
    "    recurrent_dropout=HP.rec_dropout, name = \"LSTM_encoder\"), merge_mode='concat', name = \"BI_LSTM_encoder\")(encoder_input)\n",
    "\n",
    "hidden_state_mean = tf.keras.layers.Dense(HP.latent_dim, activation='linear', name = \"mean_MLP\")(encoderLSTM)\n",
    "\n",
    "hidden_state_variance = tf.keras.layers.Dense(HP.latent_dim, activation='linear', name = \"variance_MLP\")(encoderLSTM)\n",
    "# Now we load the weights from the trained model\n",
    "encoder = tf.keras.models.Model(encoder_input,[hidden_state_mean, hidden_state_variance])\n",
    "encoder.summary()\n",
    "encoder.load_weights(\"model/\"+model_name+\".h5\", by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP.data_location = \"data/carrot.npz\"\n",
    "datas = data_Manager.Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"153.14698522294654\" version=\"1.1\" width=\"86.05580456908308\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"153.14698522294654\" width=\"86.05580456908308\" x=\"0\" y=\"0\"/><path d=\"M42.11509710557741,63.109616221752376 m-2.510214242151354,-1.369207768446193 l-2.510214242151354,-0.22820129474103215 -2.510214242151354,0.0 l-4.3358246000796115,1.369207768446193 -4.3358246000796115,3.8794220105975468 l-0.9128051789641286,3.6512207158565144 0.0,5.476831073784772 l5.476831073784772,18.256103579282573 3.6512207158565144,33.3173890321907 l1.369207768446193,0.6846038842230965 1.1410064737051608,-1.369207768446193 l5.248629779043739,-9.128051789641287 8.899850494900253,-25.102142421513538 l1.597409063187225,-10.497259558087478 0.22820129474103215,-9.58445437912335 l-0.6846038842230965,-2.510214242151354 -3.423019421115482,-4.792227189561675 l-7.302441431713029,-4.792227189561675 m-1.369207768446193,4.564025894820643 l12.094668621274703,-23.04833076884425 4.107623305338579,-9.812655673864382 l5.020428484302708,-7.074240136971997 m-25.5585450109956,43.814648590278175 l12.322869916015737,-19.39711005298773 9.58445437912335,-21.679123000398057 m-20.994519116174956,34.91479809537792 l4.792227189561675,-10.269058263346446 7.074240136971997,-24.41753853729044 m-13.463876389720896,36.96860974804721 l8.215246610677157,-28.068759253146958 1.8256103579282572,-11.866467326533671 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "x = datas.train[random.randint(0, len(datas.train))]\n",
    "x_2 = np.expand_dims(x, axis = 0)\n",
    "mean, variance = encoder.predict(x_2)\n",
    "latent = np.random.rand(1,HP.latent_dim)*np.exp(variance/2)+mean\n",
    "draw.draw_strokes(x, svg_filename=\"results/carrot_original_4.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of sketch\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"157.3655606872861\" version=\"1.1\" width=\"107.00548414347784\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"157.3655606872861\" width=\"107.00548414347784\" x=\"0\" y=\"0\"/><path d=\"M34.276131139624795,63.551000217318986 m0.5613703085547156,2.582061976457054 l0.2835920324658839,22.633513127888563 1.4515604819902308,8.595012368913517 l4.5525319330554295,18.923789633380544 6.0789592097149425,12.585623347317107 l3.2374621029944795,3.4945600160103245 2.467772599282721,-2.4293580303887072 l1.548721434287071,-1.1790727731636257 2.5773428940111995,-3.9919201896643006 l2.371085802647722,-4.827842584082787 2.7589254262169383,-14.503463233926052 l0.11004798655755627,-15.214497241630573 -3.293232193089555,-21.972215211653236 l-2.6267249915257174,-7.886641254903018 -2.5480235066365347,-2.6959109925559748 l-3.24338106599918,0.9966608736031501 -9.340174601070421,1.039921860741229 l-8.165618966766194,-0.7608925509391162 -1.879161257380924,-0.2603791446805391 l-1.4584024565581293,-0.26679762997147555 -1.57894241875756,-1.6861363835332333 l-2.07427618342204,-4.67622216030774 -1.0675657101974325,-8.967866426957224 l0.462710118976778,-8.117677087565065 1.8365859172830612,-5.788308387703962 l2.6694221688347066,-2.869097749493091 2.330941380414289,-0.7484696538324975 l2.9991002175056214,0.5965068248617925 2.335278170315652,2.305368294105893 l1.65369075821505,6.202180758358393 0.24027042380572344,8.621716947285766 l-0.5814039261528597,-11.697608511092326 1.2349379128699263,-3.6312160592777554 l4.900061635543951,-2.9563229889200002 3.605608718910931,0.2099851562577812 l2.299861374112865,3.062013979296141 1.1127172662765854,7.862378450191288 l-0.46685811922583476,7.540868560269366 1.4886829110122373,-4.312742856241048 l2.7161364566119075,-3.8620062091199374 3.3291849667124453,-2.558502019809643 l4.049552724658916,-3.08427950994441 2.8194172007293554,-1.1779912970771778 l2.9235081744477673,0.15477116630738477 1.6075154326350443,2.5949397950030137 l0.5047802257584226,4.607790095299759 -1.4699816076943495,4.262865577964854 l-6.297736605283311,8.129271564217209 4.9154986857390455,-2.5254999974253183 l5.597484320276554,-2.4481472901093024 4.759153955643127,0.5504548634267533 l2.1785558845162427,0.23742328318632294 1.250807400027998,1.5030836997319645 l-0.5804124142456751,4.596245636754003 -4.83814466419013,7.807429712331526 l-4.784597790374172,3.533243129982199 -2.829845968540338,-0.03620413086976054 m-28.95391343849324,14.53163179135009 l10.501789458848908,1.5148851832222987 9.740245932156222,-1.1233963890836378 m-20.765293491360833,11.167608487720356 l15.811216452538332,4.652152197301691 7.367601871115,-0.2728562217505384 m-20.3886450196017,12.55552216198118 l5.352131967442767,0.02662419407319705 10.62151203216224,-2.9671934949957204 m-11.883790812282392,11.567770743424756 l10.536312013363554,1.6358487932555226 4.454472330240071,-0.13166259057002097 m0.0,0.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq =  g.generate_sketch(generator, latent_to_hidden_state_model, temperature=0.2)\n",
    "draw.draw_strokes(seq, svg_filename=\"results/carrot_from_IID_4.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
